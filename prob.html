<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Probability and Statistics</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/black.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
					<h1>Probability and Statistics</h1>
				</section>
				<section>
					<h2>Random Experiment</h2>
					<p>An experiment for which we cannot predict the outcome in advance.
					</br>
					Example: Coin Tossing
					</p>
				</section>
				<section>
					<h2>Are they really unpredictable?</h2>
					<p style="text-align: justify;">
						In some cases, no! For instance, if we know the exact position of the coin at the moment that it is being flipped, and also we are aware of all the forces having an impact on this procedure, we can exactly predict what is going to happen.
					</br>
					However, we usually have much less knowledge about the experiment and we would like to model the experiment using all we have.
					</p>
				</section>
				<section>
					<h2>Are they really unpredictable?</h2>
					<p>
						In some cases, yes! Quantum mechanical processes are an example of this. Read More: <a href="shorturl.at/auJPZ">shorturl.at/auJPZ</a>
			
					</br>
					Remember, we are trying to model the experiment using "ALL WE HAVE"!
					</p>
				</section>
				<section>
					<h2>Sample Space</h2>
					The set of all possible outcomes for an experiment</br>
					Example: When you toss a coin, the sample space is 
					\[
					\Omega=S=\{\mathrm{Head, Tail}\} \]
				</section>
				<section>
					Another example: When rolling a dice, we have 
					\[
					S = \{1,2,3,4,5,6\}
					\]
				
				In this case, we can consider "the outcome being an odd number" an "event".
				</section>
				<section>
					<h2>Event</h2>
					An event is a subset of the sample space.
				</br>
				Example: "the outcome being an odd number" is equivalent to having \[E=\{1,3,5\}\]
				
				Obviously we have \[E\subseteq S\]
				</section>
				<section>
					<h2>
						Probability Function
					</h2>
					A function $P:A\mapsto [0,1]$
					where $A$ is an event. The following properties should hold for $P$:<br><br>
					<ul>
						<li>
							$\forall A\subset\Omega \quad 0\le P(A)\le 1$
						</li>
						<li>
							$P(\Omega)=1$
						</li>
						<li>
							If $A$ and $B$ are disjoint ($A \cap B=\emptyset$), then we have: $P(A \cup B)=P(A)+P(B)$
						</li>
						</ul>
				</section>
				<section>
					<h2>Some properties of the probability function</h2>
						$$P(A^C)=1 - P(A) $$
						Proof: 

						$$(\Omega = A \cup A^C) \land (A \cap A^C = \emptyset)$$ $$\implies 1 = P(\Omega) = P(A \cup A^C) = P(A) + P(A^C)$$
						$$\implies 1 = P(A)+P(A^C)$$
				</section>
				<section>
					<h2>Some properties of the probability function</h2>
					$$P(A \cup B) = P(A)+P(B)-P(A \cap B)$$
					Proof: 
					$$ (i): P(A \cup (B \cap A^C)) = P(A) + P(B \cap A^C)$$
					$$ \implies P(A \cup B) = P(A) + P(B \cap A^C) $$
				</section>
				<section>
					<h2>Some properties of the probability function</h2>
					$$P(A \cup B) = P(A)+P(B)-P(A \cap B)$$
					Proof: 
					$$ (ii): P((B \cap A) \cup (B \cap A^C)) = P(B \cap A) + P(B \cap A^C) $$
					$$ \implies P(B \cap A^C) = P(B) - P(A \cap B) $$
				</section>
				<section>
					<h2>Conditional Probability</h2>
					$$P(A|B) := \dfrac{P(A \cap B)}{P(B)}$$
					Note: This is only defined if $P(B)>0$.
				</section>
				<section>
					<h2>Is it really a probability function?</h2>
				</section>
				<section>
					$$ 0 \le P(A \cup B) \le 1 \newline P(B) >0 \newline \implies P(A|B) \in [0,1]$$
				</section>
				<section>
					$$ \Omega = B \implies P(\Omega|B) = P(B|B) = \dfrac{P(B \cap B)}{P(B)} = 1$$
				</section>
				<section>
					$$ A_1 \cap A_2 = \emptyset \implies P(A_1 \cup A_2 | B) = \dfrac{P((A_1 \cup A_2)\cap B)}{P(B)} \newline = \dfrac{P((A_1 \cap B) \cup (A_2 \cap B))}{P(B)} = \dfrac{P(A_1 \cap B)+P(A_2 \cap B)}{P(B)} \newline \dfrac{P(A_1 \cap B)}{P(B)} + \dfrac{P(A_2 \cap B)}{P(B)} \newline = P(A_1 | B) + P(A_2 | B)$$
				</section>
				<section>
					<h2>Bayes Theorem</h2>
					$$P(A|B) = \dfrac{P(B|A)P(A)}{P(B)}$$
					Proof:
					$$ P(A|B) = \dfrac{P(A \cap B)}{P(B)} \implies P(A|B)P(B) = P(A \cap B) \newline P(B|A) = \dfrac{P(B \cap A)}{P(A)} \implies P(B|A)P(A) = P(A \cap B)$$
				</section>
				<section>
					<h2>Partition of a set</h2>
					A family of sets $P$ is a partition of $S$ if and only if all of the following conditions hold:
					<br><br>
					<ol>
						<li>$\emptyset \notin P$</li>
						<li>$\cup_{X\in P} X = S$</li>
						<li>$\forall A,B \in P \quad A\cap B = \emptyset$</li>
					</ol>
				</section>
				<section>
					<h2>The Law of Total Probability</h2>
					Assume that $\Omega$ is partitioned into $B_i$'s. Then we have:<br>
					$$ P(A) = \sum_i P(A\cap B_i) = \sum_i P(A|B_i)P(B_i)$$
					Proof:<br>
					$$
					A = A \cap \Omega \implies P(A) = P(A\cap \Omega) \stackrel{\text{$ \Omega = \cup_i B_i $}}{\implies}
					\newline P(A) = P(A\cap (\cup_i B_i)) = P(\cup_i (A\cap B_i)) \stackrel{\text{$ \forall i,j (A\cap B_i)\cap (A\cap B_j) = \emptyset $}}{=}
					\newline \sum_i P(A\cap B_i) = \sum_i P(A|B_i) P(B_i)
					$$
				</section>
				<section>
					<h2>Independent events</h2>
					Two events $A$ and $B$ are called independent if one of these hold:<br><br>
					<ul>
						<li>$P(B|A) = P(B)$ and $P(A|B)=P(A)$ </li>
						<li>$P(A)=0$ or $P(B)=0$</li>
					</ul>
				</section>
				<section>
					<h2>Independent Events</h2>
					$$
					P(A|B) = P(A) \land P(B|A) = P(B)
					\newline
					\implies P(A\cap B) = P(A|B)P(B) = P(A)P(B)
					\newline \implies P(A \cap B) = P(A)P(B)
					$$
				</section>
				<section>
					<h2>Independent Events given another Event</h2>
					$$ P(A|B,C) = P(A|C) \land P(B|A,C) = P(B|C) 
					$$
				</section>
				<section>
					<h2>Independent Events given another Event</h2>
					$$  
					P(A|B,C) = P(A|C) \iff \frac{P(A\cap B \cap C)}{P(B\cap C)} = 
					\frac{P(A\cap C)}{P(C)}
					\newline \iff
					\frac{P(A\cap B \cap C)}{P(A\cap C)} = \frac{P(B\cap C)}{P(C)} 
					\newline \iff
					P(B|A,C) = P(B|C)
					$$
				</section>
				<section>
					<h2>Chain Rule</h2>
					$$
					P(A_1 \cup A_2 \cup \dots \cup A_k)
					=
					P(\cup_{i=1}^k) = \newline
					\sum_{i=1}^k P(A_i) - \sum_{i,j} P(A_i \cap A_j)
					+ \sum_{i,j,k} P(A_i \cap A_j \cap A_k) - \dots
					$$
				</section>
				<section>
					<h2>Chain Rule</h2>
					$$
					P(A_1 \cap A_2 \cap \dots \cap A_k)
					= \newline
					P(A_1)P(A_2 | A_1)P(A_3| A_2, A_1)
					P(A_4 | A_3,A_2,A_1) \dots \newline
					P(A_k|A_{k-1}, A_{k-2}, \dots, A_1)
					$$
				</section>
				<section>
					<h2>Random Variable</h2>
					A function $X:\Omega\to\mathbb{R}$ which takes an outcome of a random experiment as its input, <br>and outputs a real number.
					<br><br>
					Example: $$ X(\mathrm{Heads}) = 0 \newline X(\mathrm{Tails}) = 1$$
				</section>
				<section>
					<h2>Discrete vs. Continuous</h2>
					If the range of the R.V. is countable, it's called discrete.<br>
					If it is uncountably infinite, (usually an interval), it's continuous.
				</section>
				<section>
					<h2>Discrete Random Variable</h2>
					Experiment: Rolling a dice
					<br>
					For each face $f$, define $X(f)=f+1$
				</section>
				<section>
					<h2>Continuous Random Variable</h2>
					Experiment: Exploring the Status of the Weather
					<br> Random Variable: The Temperature
					<br>
					<br>Note: A random variable actually simplifies the complicated situation of the outcome of an experiment such as the position of the molecules present in the air, the status of the sun, etc. 
				</section>
				<section>
					<h2>Probability Mass Function (PMF)</h2>
					Assume that $R_X$ is the range of an R.V. like $X$.<br>
					A function $p_X:R_X\to [0,1]$ for which the following conditions hold:
					<br><br><ul>
						<li>$\forall x \quad p_X(x)\ge 0$</li>
						<li>$\sum_{x \in \Omega_X} p_X(x) = 1$</li>
					</ul>
				</section>
				<section>
					<h2>Probability Mass Function (PMF)</h2>
					$$
					X(\mathrm{Heads}) = 0 \quad \land \quad X(\mathrm{Tails}) = 1 \implies R_X = \{0,1\}
					\newline
					p(0) = \frac{1}{2} \newline
					p(1) = \frac{1}{2}
					$$
				</section>
				<section>
					<h2>Probability Mass Function vs. Probability Function</h2>
					PMF: $\quad p:\Omega \to [0,1]$<br>
					Probability Function: $\quad \mathbb{P}:F \to [0,1]$
				</section>
				<section>
					<h2>Probability Mass Function vs. Probability Function</h2>
					PMF: $\quad p:\mathrm{outcome} \mapsto r\in [0,1]$<br>
					Probability Function: $\quad \mathbb{P}:\mathrm{event} \mapsto r\in [0,1]$
				</section>
				<section>
					<h2>Cumulative Distribution Function (CDF)</h2>
					$$
					\forall x \in R_X \quad F_X(x) = \mathbb{P}(\{\omega\in \Omega:X(\omega) \le x\}) = \mathbb{P}(X \le x)
					$$
				</section>
				<section>
					<h2>CDF is Non-decreasing</h2>
					<ul>
						<li>
							$\lim_{x\to-\infty} F(x)=0$
							<br>
							Since: $F(-\infty)=\mathbb{P}(X \le -\infty)=0$
						</li><br>
						<li>
							$\lim_{x\to+\infty} F(x)=1$
							<br>
							Since: $F(+\infty)=\mathbb{P}(X \le +\infty)=1$
						</li><br>
						<li>
							$x_1 \le x_2 \implies F(x_1) \le F(x_2)$
						</li>
					</ul>
				</section>
				<section>
					<h2>Question</h2>
					Assume 1,000,000 people, each paying 10,000 dollars are participating in a game.
					<br>At the end, only one person will win, and the reward will be 100,000,000 dollars. Is it wise to participate?
				</section>
				<section>
					<h2>Probability Approach</h2>
					Experiment: Measuring the change in our money after participating in the game.
					<br>
					Outcome: The amount of change
					<br>
					Random Variable: The amount of change <br>$X(i)=i \quad \forall i$
				</section>
				<section>
					We have two cases: 1. Winning 2. Losing
					<br>
					$$
					\mathbb{P}(\mathrm{Winning}) = \frac{1}{1,000,000}
					$$
					If we win, the change will be:<br>
					-10,000 + 100,000,000 = 99990000
				</section>
				<section>
					We have two cases: 1. Winning ~100M 2. Losing 10K
					<br>
					$$
					\mathbb{P}(\mathrm{Losing}) = \frac{999999}{1000000}
					$$
					If we lose, the change will be:<br>
					-10,000
				</section>
				<section>
					We calculate a weighted average of the outcome in these two cases.
					<br>
					The weight of each case is the probability of it.
					<br>
					$$
					\sum_{x \in R_X} x\mathbb{P}(x) = \newline 99990000\times \frac{1}{1000000} + (-10000)\times \frac{999999}{1000000}
					\newline = -9900
					$$
					So, on average, we will lose!
				</section>
				<section>
					<h3>Expected Value of a Discrete R.V.</h3>
					$$
					E(X) = \sum_{x\in \mathrm{Range}(X)}x\mathbb{P}(X=x)
					$$
				</section>
				<section>
					<h2>Example</h2>
					<p style="text-align:left">
					$$
					X: \mbox{the result of rolling a die}
					\newline
					Y: 3X+1
					\newline
					E(Y)=\sum_{y\in R_Y}\mathbb{P}(Y=y)y \newline = \frac{1}{6}(4+7+10+13+16+19) = \frac{23}{2}
					$$
					Note that $4=3*1+1$, $7=3*2+1$, $\dots$ 
					</p>
				</section>
				<section>
					<h3>Expected Value of a Linear Transformation of a R.V.</h3>
					$$
					Y=aX+b \implies E(Y)=aE(X)+b
					\newline
					E(X)=\sum x\mathbb{P}(X=x)
					\newline
					Y=aX+b \implies E(Y)= \sum (ax+b)\mathbb{P}(Y=ax+b) \newline = \sum (ax+b)\mathbb{P}(X = x)
					
					 = \sum ax \mathbb{P}(X = x) + \sum b \mathbb{P}(X = x)
					 \newline
					 = a \sum x \mathbb{P}(X=x) + b \sum \mathbb{P}(X=x) = aE(X)+b 
					$$
				</section>
				<section>
					<h2>Variance</h2>
					$$
					Var(X) = E(X-E(X))^2 = \sum_{x\in R_X}(x-E(X))^2 \mathbb{P}(X=x)
					$$
					Why the power of 2?	<br>
					$E(X) \mbox{ is constant} \newline \implies E(X-E(X))=E(X)-E(X)=0$
				</section>
				<section>
					<h2>Example</h2>
					The variance of $X$ being the result of rolling a die
					<br>
					It is $\frac{70}{24}$, and this shows how much the result can get further from its expected value!
				</section>
				<section>
					<h2>Standard Deviation</h2>
					$$
					\sqrt{Var(X)} = SD(X)
					$$
					Note: The variance is not in the same unit of measurement as the original data. Hence, SD is needed.
				</section>
				<section>
					<h2>Variance - Another Formula</h2>
					$$
					Var(X) = E[(X-E(X))^2] = \sum_{x\in R_X}\mathbb{P}(X=x)(x-\mu)
					\newline
					= \sum \mathbb{P}(X=x)(x^2+\mu^2-2x\mu)
					\newline
					=\mu^2\sum\mathbb{P}(X=x)-2\mu\sum x \mathbb{P}(X=x) + \sum x^2 \mathbb{P}(X=x)
					\newline
					= \mu^2-2\mu^2+E(X^2)
					\newline
					\implies Var(X) = E(X^2)-E(X)^2
					$$
				</section>
				<section>
					<h2>Variance of Linear Combination of a R.V.</h2>
					$$
					Var(aX+b) = E[(aX+b-a\mu-b)^2] = E[a^2(X-\mu)^2] \newline = a^2E[(X-\mu)^2] = a^2 Var(X)
					$$
				</section>
				<section>
					<h3>Meaning of a Discrete Distribution</h3>
					<br>A function distributing the probability between the various cases
					<br>
					Recap: That was called a Probability Mass Function which gave a probability for the occurence of each outcome of a discrete random variable
				</section>
				<section>
					<h2>Well-known discrete distributions</h2>
				</section>
				<section>
					<h3>Bernoulli Distribution</h3>
					$$
					X(\mathrm{success}) = 1
					\quad X(\mathrm{failure}) = 0
					$$

					$$
					\begin{cases}
					\mathbb{P}(X=1) = p \newline
					\mathbb{P}(X=0) = q = 1-p
					\end{cases}
					\implies X \sim \mathrm{Bernoulli}(p)
					$$
					<br>Example: The outcome of tossing a coin
				</section>
				<section>
					$$
					E(X) = 0*(1-p) + 1*p = p
					$$
					<br>
					$$
					Var(X) = E(X^2) - (E(X))^2 \newline = [0^2*(1-p) + 1^2*p] - p^2
					\newline
					= p-p^2 = p(1-p) = pq
					$$
				</section>
				<section>
					<h2>Cumulative Distribution Function</h2>
					<img src="img/bernoulli-cdf.jpg" style="width:80%">
				</section>
				<section>
					<h3>Binomial Distribution</h3>
					Repeat a Bernoulli Experiment $n$ times<br>
					Define $Y$ as the number of times we succeed<br>
					$$
					\mathbb{P}(Y=k) = {n\choose k}p^k (1-p)^{n-k} \quad 0 \le k \le n
					\newline \implies Y \sim \mathrm{Binomial}(n,p)
					$$
				</section>
				<section>
					Define $X_i$'s as the outcomes of $n$ independent Bernoulli Experiments
					$$
					E(X_1 + X_2 + \dots + X_n) = E(X_1) + E(X_2) + \dots + E(X_n)
					\newline
					Y=X_1 + X_2 + \dots + X_n
					\implies E(Y) = \sum_{i=1}^n E(X_i) = np
					\newline Var(Y) = Var(X_1) + Var(X_2) + \dots + Var(X_n)
					\newline \implies Var(Y) = npq
					$$
				</section>
				<section>
					<h2>Geometric Distribution</h2>
					Assume that we repeat the Bernoulli experiment again and again. <br>
					Let $p$ be the probability of success, and $1-p$ be that of failure for each experiment.<br>
					How many times should we do this until we succeed? 
				</section>
				<section>
					<h2>Q: What is the probability of being successful at the $k$'s experiment for the first time?</h2>
				</section>
				<section>
					<h2>Geometric Distribution</h2>
					$X$: the index of the first time we succeed while repeating a Bernoulli experiment 

					$$
					\mathbb{P}(X=k) = (1-p)^{k-1} p
					\newline
					E(X) = \sum_{k=1}^\infty k(1-p)^{k-1}p
					$$
				</section>
				<section>
					<h2>Geometric Distribution</h2>
					$$
					E(X) = 1\times p + 2\times(1-p)^1 p + 3\times (1-p)^2 p + \dots 
					\newline 
					 = [p + (1-p)p + (1-p)^2 p + \dots] \newline + [(1-p)p + (1-p)^2 p + \dots] \newline + [(1-p)^2 p + (1-p)^3 p + \dots] + \dots 
					\newline
					= \frac{p}{1-(1-p)} + \frac{p(1-p)}{1-(1-p)} + \frac{p(1-p)^2}{1-(1-p)} + \dots 
					$$
				</section>
				<section>
					<h2>Geometric Distribution</h2>
					$$
					E(X) = 1 + \frac{p(1-p)}{p} + \frac{p(1-p)^2}{p} + \dots 
					\newline
					= 1 + \frac{\frac{p(1-p)}{p}}{1-(1-p)} = 1 + \frac{1-p}{p} = \frac{p+1-p}{p} = \frac{1}{p}
					$$
				</section>
				<section>
					<h2>
						Hypergeometric Distribution
					</h2>
					Assume we have $N_1$ red balls and $N_2$ blue balls inside a bag. We take $n$ balls out of the bag.
					<br>
					What is the probability of $x$ balls being red?
					<br>
					$X$: the number of red balls we take out of the bag
					$$
					\mathbb{P}(X=x) = \frac{{N_1 \choose x}{N_2 \choose {n-x}}}{{N_1+N_2}\choose n}
					$$
				</section>
				<section>
					<h2>Hypergeometric Distribution</h2>
					$$
					X_i := 
					\begin{cases}
					1 \quad \mathrm{red} \newline
					0 \quad \mathrm{blue}
					\end{cases}
					
					\implies X = X_1 + X_2 + \dots + X_n
					\newline
					E(X) = E(X_1) + E(X_2) + \dots + E(X_n) = n(\frac{N_1}{N_1+N_2})
					$$
				</section>
				<section>
					<h2>Poisson Distribution</h2>
					<p style="text-align: justify;">
					Assume that you are standing on a street watching the cars.<br>
					Also, assume that the drivers are not aware of what you are doing, and are independently driving their cars.
					Moreover, assume that the time between the arrivals of the cars has a fixed rate.<br>
					Each time a car passes by, you press a button.<br>
					After a fixed amount of time, count the times you have pressed the button.<br>
					</p>
				</section>
				<section>
					<h2>Poisson Distribution</h2>
					Q: What is the probability that we see $k$ cars in a fixed amount of time?
					<br>
					<img src="img/poisson.png">
				</section>
				<section>
					<img src="img/poisson.png">
					$$
					\mathbb{P}(X=k) = \lim_{n \to \infty} {n \choose k}(\frac{\lambda}{n})^k (1-\frac{\lambda}{n})^{n-k}
					\newline
					\lim_{n\to \infty} \frac{n!}{k! (n-k)!} \frac{\lambda ^ k}{n ^ k}(1-\frac{\lambda}{n})^n
					$$
				</section>
				<section>
					$$
					\mathbb{P}(X=k) = \lim_{n \to \infty} {n \choose k}(\frac{\lambda}{n})^k (1-\frac{\lambda}{n})^{n-k}
					\newline
					\lim_{n\to \infty} \frac{n!}{k! (n-k)!} \frac{\lambda ^ k}{n ^ k}(1-\frac{\lambda}{n})^n
					\newline
					\lim_{n \to \infty} \frac{\lambda^k}{k!}\frac{[n(n-1)(n-2)\dots(n-k+1)]}{n^k}(1-\frac{\lambda}{n})^n (1-\frac{\lambda}{n})^{-k}
					\newline
					\lim_{n \to \infty} \frac{\lambda ^k}{k!}[1.(1-\frac{1}{n})(1-\frac{2}{n})\dots (1-\frac{k+1}{n})] (1-\frac{\lambda}{n})^n (1-\frac{\lambda}{n})^{-k}
					$$
				</section>
				<section>
					$$
					\lim_{n \to \infty} \frac{\lambda ^k}{k!}[1.(1-\frac{1}{n})(1-\frac{2}{n})\dots (1-\frac{k+1}{n})] (1-\frac{\lambda}{n})^n (1-\frac{\lambda}{n})^{-k}
					\newline 
					= \frac{\lambda^k}{k!} \times 1 \times e^{-\lambda} \times 1^{-k} = \frac{\lambda^k}{k!}e^{-\lambda}
					\newline \implies \mathbb{P}(X=k) = \frac{e^{-\lambda} \lambda^k}{k!}
					$$
				</section>
				<section>
					<h2>Poisson Distribution</h2>
					<p style="text-align: justify;">
					$X$: The number of events occuring in a given time interval
					<br><br>
					$\mathbb{P}(X=k) = \frac{e^{-\lambda} \lambda^k}{k!} \implies X \sim \mathrm{Poisson}(\lambda)$
					<br><br>
					Note: $E(X)=\lambda \quad$ (Proof: Google!)
					</p>
				</section>
				<section>
					<h3>Linear Property of Expected Value</h3>
					$$
					E(aX+b)=aE(X)+b \quad g(X):=aX+b
					\newline
					E(aX+b)=E(g(X))=\int_{-\infty}^{\infty}(ax+b)f_X(x)dx
					\newline = \int ax f_X(x)dx + \int b f_X(x)dx \newline = a\int x f_X(x)dx + b \int f_X(x)dx = aE(X) + b
					$$
				</section>
				<section>
					<h2>Linear Transformation Impact on Variance</h2>
					$$
					Var(aX+b) = E[(aX+b-a\mu -b)^2]\newline = E(a^2 (X-\mu)^2) \newline = a^2 E[(x-\mu)^2] = a^2 Var(X)
					$$
				</section>
				<section style="text-align: justify;">
					<h2>Normal Distribution</h2>
					Assume you have a coin. Toss it for a lot of times, and calculate the mean.<br>
					Repeat this experiment many times.<br>
					The set of the means you calculate follow a distribution.
				</section>
				<section style="text-align: justify;">
					<h2>Normal Distribution</h2>
					If we have a lot of independent random variables all following a same distribution, <br>
					the sum (or mean) of them follows a Normal (Gaussian) Distribution.
					$$
					X_i\mbox{'s are independent and follow the same distribution}\newline \implies X_1 + X_2 + \dots + X_n \sim \mathcal{N}(\mu, \sigma^2)
					$$
				</section>
				<section>
					<h2>Normal Distribution</h2>
					$$
					X \sim \mathcal{N}(\mu, \sigma^2) \implies f_X(x)= \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
					$$
				</section>
				<section>
					<h2>Standard Normal Distribution</h2>
					$$
					X \sim \mathcal{N}(0, 1) \implies f_X(x)= \frac{1}{\sqrt{2\pi }} e^{-\frac{x^2}{2}}
					$$
				</section>
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
		</script>
		<script src="plugin/math/math.js"></script>
		<script>
		  Reveal.initialize({
			math: {
			  mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
			  config: 'TeX-AMS_HTML-full',
			  // pass other options into `MathJax.Hub.Config()`
			  TeX: { Macros: { RR: "{\\bf R}" } }
			},
			plugins: [ RevealMath ]
		  });
		  Reveal.initialize({ slideNumber: 'c/t' });

		</script>
	</body>
</html>
